{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Environmental sound classification with semi-supervised learning\n",
    "This is the complimentary notebook for the handed in paper. This notebook shows the implementation of the different architecture and the obtained results. \n",
    "\n",
    "### Organization of notebook\n",
    "The notebook is organized into a data and pre-processing section, a model section, a training section and finally a results/visualization section.\n",
    "\n",
    "First, we import all of the used libraries and set a random seed for reproducability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Input, Dense, Flatten, Reshape,\\\n",
    "    Activation, Dropout, Conv2D, Conv2DTranspose,\\\n",
    "    MaxPooling2D, BatchNormalization, UpSampling2D, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "import pescador\n",
    "import matplotlib\n",
    "\n",
    "# Reproducability\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and pre-processing\n",
    "All of the data-loading and pre-processing is implemented in the DataHandler.py file. The raw sound files are loaded and pre-processed into a Log-scaled mel-spectrogram of size $\\mathbf{x}_n \\in \\mathbb{R}^{28 \\times 36}$. Either read the data section in the paper or inspect the DataHandler class for more details. The DataHandler class is used as a stream of batches. It takes parameters such as window size, hop-length, and bands.\n",
    "\n",
    "#### Datasets\n",
    "For this notebook, we focus on the ESC-US (250.000 unlabeled environmental sounds) and the ESC-50 (2000 labeled environmental sounds. Hence, we have two data streamers, one for each dataset. To run the code, you will need to download the datasets from links (see README.md) and place them into a `./datasets/` folder.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-60aa3fd337d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                            \u001b[0mval_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                            \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Means use the rest for train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                            \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ESC-US\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                            )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programming_projects/02456-deep-learning-project12/DataHandler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, bands, hop_length, window, use_delta, val_size, test_size, train_size, normalize, label_whitelist)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Loads the filenames into the above lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Track cool stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programming_projects/02456-deep-learning-project12/DataHandler.py\u001b[0m in \u001b[0;36mload_data_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mtemp_files_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_files_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programming_projects/02456-deep-learning-project12/DataHandler.py\u001b[0m in \u001b[0;36mcreate_splits\u001b[0;34m(self, file_list)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlast_test_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_test_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlast_val_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "from DataHandler import DataHandler\n",
    "\n",
    "#### ESC-US ####\n",
    "# Unlabeled datahandler\n",
    "total = 50000\n",
    "val_size = total / 5\n",
    "data_handler = DataHandler(bands=28, test_size=16, \n",
    "                           val_size=val_size, \n",
    "                           train_size=None, # Meaning use the rest for train\n",
    "                           dataset=\"ESC-US\", hop_length=6*1024, window=12*1024,\n",
    "                           )\n",
    "\n",
    "#### ESC-50 ####\n",
    "# labeled datahandler\n",
    "l_total = 2000\n",
    "l_test_size = l_total // 5\n",
    "l_val_size = (l_total - l_test_size) // 5\n",
    "labeled_data_handler = DataHandler(bands=28, val_size=l_val_size, test_size=l_test_size,\n",
    "                                   train_size=l_train_size,\n",
    "                                   dataset=\"ESC-50\", hop_length=6*1024, window=12*1024,\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models \n",
    "In this notebook, we show the two convolution autoencoders. The difference lies in the invertion of the max-pooling layer. We also include the implementation of extending the pre-trained autoencoders to classifiers as in the semi-supervised approach, and the fully supervised model. \n",
    "\n",
    "\n",
    "#### Custom layers\n",
    "Since neither tensorflow or keras supported the unpooling layer using the stacked-what-where approach, we had to implement this ourselves. The DePool2D class takes the size of the input, size of the max-pool operation we are inverting, and the layer it should be connected to. This is seen below.  \n",
    "\n",
    "NOTE: The costum dePool2D is only supported by using theano backend to keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, multiply\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "\n",
    "def ZeroPad2D(**kwargs):\n",
    "    def call(inputs, data_format=None):\n",
    "        from keras import backend as K\n",
    "        \n",
    "        y, goal_y = inputs\n",
    "        shape = K.int_shape(y)\n",
    "        goal_shape = K.int_shape(goal_y)\n",
    "        a, b = goal_shape[-2] - shape[-2], goal_shape[-1] - shape[-1]\n",
    "        y_pad = K.spatial_2d_padding(y, padding=((0, a), (0, b)), data_format=data_format)\n",
    "        return y_pad\n",
    "\n",
    "    return Lambda(call, output_shape=lambda x: x[1], arguments=kwargs)\n",
    "\n",
    "def Where():\n",
    "    def getwhere(inputs):\n",
    "        \"\"\"\n",
    "        Calculate the 'where' mask that contains switches indicating which\n",
    "        index contained the max value when MaxPool2D was applied.  Using the\n",
    "        gradient of the sum is a nice trick to keep everything high level.\n",
    "        \"\"\"\n",
    "        from keras import backend as K\n",
    "        y_prepool, y_postpool = inputs\n",
    "        return K.gradients(K.sum(y_postpool), y_prepool)\n",
    "    \n",
    "    return Lambda(getwhere, output_shape=lambda x: x[0])\n",
    "\n",
    "def DePool2D(*args, data_format=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Uses a switch variable as described in https://arxiv.org/pdf/1311.2901.pdf\n",
    "    Can only invert maxpooling where `size = stride`.\n",
    "    \"\"\"\n",
    "\n",
    "    def call(inputs):\n",
    "        y_prepool, y_postpool, x = inputs\n",
    "        where = Where()([y_prepool, y_postpool])\n",
    "        y = UpSampling2D(*args, data_format=data_format, **kwargs)(x)\n",
    "        y = ZeroPad2D(data_format=data_format)([y, where])\n",
    "        return multiply([y, where])\n",
    "    \n",
    "    return call\n",
    "\n",
    "def Upsampling2DPadded(*args, data_format=None, **kwargs):\n",
    "    def call(inputs):\n",
    "        prepool, postpool, x = inputs\n",
    "        y = UpSampling2D(*args, data_format=data_format, **kwargs)(x)\n",
    "        y = ZeroPad2D(data_format=data_format)([y, prepool])\n",
    "        return y\n",
    "\n",
    "    return call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Convolutional Autoencoder\n",
    "The convolutional autoencoder is implemented below. It takes a configuration dictionary, where all of the hyper-parameters are declared. If the configuration settings conf.use_depool value is set to true, a custom unpooling layer is used. If not, the Upsampling2DPadded layer is used, which is also implemented under Custom layers section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional autoencoder\n",
    "def deeper_nodense(conf):\n",
    "    num_filters = conf.num_filters or 64\n",
    "    # Encoder\n",
    "\n",
    "    unpool = DePool2D if conf.use_depool else Upsampling2DPadded\n",
    "\n",
    "    kwargs = { 'data_format': \"channels_first\" }\n",
    "    pool_size = 2\n",
    "\n",
    "    # (64)5c-2p-(64)3c-2p-(64)3c-2p-10fc\n",
    "    x = Input(shape=conf.original_img_size)\n",
    "\n",
    "    conv1 = conv2d(num_filters, 5, name=\"conv1\", use_batch_normalization=conf.use_batch_normalization, **kwargs)(x)\n",
    "    maxp1 = MaxPooling2D(pool_size=(pool_size, pool_size), **kwargs)(conv1)\n",
    "    conv2 = conv2d(num_filters, 3, name=\"conv2\", use_batch_normalization=conf.use_batch_normalization, **kwargs)(maxp1)\n",
    "    \n",
    "    if conf.num_layers == 3:\n",
    "        maxp2 = MaxPooling2D(pool_size=(pool_size, pool_size), **kwargs)(conv2)\n",
    "        conv3 = conv2d(num_filters, 3, name=\"conv3\", use_batch_normalization=conf.use_batch_normalization, **kwargs)(maxp2)\n",
    "        maxp3 = MaxPooling2D(name=conf.encoded_name, pool_size=(pool_size, pool_size), **kwargs)(conv3)\n",
    "        z = maxp3\n",
    "    else:\n",
    "        maxp2 = MaxPooling2D(name=conf.encoded_name, pool_size=(pool_size, pool_size), **kwargs)(conv2)\n",
    "        z = maxp2\n",
    "        \n",
    "    y = z\n",
    "    if conf.num_layers == 3:\n",
    "        y = unpool(size=(pool_size, pool_size), **kwargs)([conv3, maxp3, y])\n",
    "        y = deconv2d(num_filters, 3, **kwargs)(y)\n",
    "    \n",
    "    y = unpool(size=(pool_size, pool_size), **kwargs)([conv2, maxp2, y])\n",
    "    y = deconv2d(num_filters, 3, use_batch_normalization=conf.use_batch_normalization, **kwargs)(y)\n",
    "    y = unpool(size=(pool_size, pool_size), **kwargs)([conv1, maxp1, y])\n",
    "    y = deconv2d(1, 5, use_batch_normalization=conf.use_batch_normalization, activation=None, **kwargs)(y)\n",
    "\n",
    "    # AE\n",
    "    ae = Model(inputs=x, outputs=y)\n",
    "    optimizer = Adam(lr=conf.learning_rate, decay=conf.decay)\n",
    "    ae.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    encoder = Model(x, z)\n",
    "\n",
    "    return ae, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Pre-trained weights classifier and supervised classifier\n",
    "This model takes an autoencoder as input, extracts the encoder part, and then extends it with two fully connected layers. The first is of 128 hidden units and the next one is the classification layer with 50 output units and a softmax activation function. The autoencoder is already trained, if we are building a model with pre-trained weights.\n",
    "\n",
    "This function is also used for building the fully supervised classifier with no pre-trained weights. In this case, an autoencoder is build and passed (but not pre-trained) and we are therefore sure to have two classifiers with same architecture, but one with pre-training of weights and one without. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pretrained_classifier(ae, class_conf):\n",
    "    encoded = ae.get_layer(class_conf.encoded_name).output\n",
    "    hidden_units = class_conf.hidden_units or 200\n",
    "\n",
    "    if len(class_conf.latent_dim) > 1:\n",
    "        y = Flatten(input_shape=class_conf.latent_dim)(encoded)\n",
    "        y = Dense(hidden_units)(y)\n",
    "    else:        \n",
    "        y = Dense(hidden_units, input_shape=class_conf.latent_dim)(encoded)\n",
    "    if class_conf.dropout > 0:\n",
    "        y = Dropout(class_conf.dropout)(y)\n",
    "    y = Dense(class_conf.num_classes, activation='softmax')(y)\n",
    "    \n",
    "    classifier = Model(ae.inputs, y)\n",
    "    classifier.compile(optimizer=keras.optimizers.Adam(),\n",
    "                       loss=keras.losses.categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also need of a method used to the load weights from a pre-trained autoencoder. It is very simple as below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(model, uuid):\n",
    "    model.load_weights(os.path.join(MODEL_DIR, uuid + '.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training\n",
    "We are now ready to perform the training of our models. \n",
    "\n",
    "#### Autoencoder \n",
    "First, we include the training process of our convolutional autoencoder. We already defined our batch streamers from our own implemented DataLoader.py. The DataLoader class also takes care of splits, number of updates per epoch and everything else relevant to data.  \n",
    "\n",
    "We use early stopping with a patience of 5 and only save the best performing model. The number of epochs is set in the configuration file (conf) when the training method is called. We return the training-history if we need to plot the evolving graphs from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ae(ae, conf, data_handler, save=True):\n",
    "    batches = data_handler.get_train_batch_streamer(conf.batch_size)\n",
    "    batches_validation = data_handler.get_validation_batch_streamer(\n",
    "        conf.batch_size)\n",
    "\n",
    "    checkpoints = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "        ]\n",
    "    \n",
    "    if save:\n",
    "        save_path = os.path.join(MODEL_DIR, conf.uuid + \".h5\")\n",
    "        checkpoints.append(ModelCheckpoint(save_path, monitor='val_loss',\n",
    "                                           save_weights_only=True,\n",
    "                                           verbose=1, mode='auto'))\n",
    "        checkpoints.append(CSVLogger(os.path.join(OUTPUT_DIR, conf.uuid + \".log\")))\n",
    "\n",
    "    history = ae.fit_generator(\n",
    "        pescador.tuples(batches, 'X', 'X'),\n",
    "        steps_per_epoch=data_handler.number_of_train_batches,\n",
    "        epochs=conf.epochs,\n",
    "        verbose=1,\n",
    "        validation_data=pescador.tuples(batches_validation, 'X', 'X'),\n",
    "        validation_steps=data_handler.number_of_val_batches,\n",
    "        callbacks=checkpoints)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is called from the following autoencoder function. If given the parameter train=False, then we return an untrained autoencoder, which is used for the supervised classifier. If not, we call the training function giving us an autoencoder with pre-trained weights. \n",
    "\n",
    "The configuration dictionary is declared here. In order to experiment with different hyper-parameters, you will need to change the values here or specify the extra_conf dictionary parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autoencoder(load=True, train=False, extra_conf={}):\n",
    "    img_shape = data_handler.get_X_shape()\n",
    "    \n",
    "    conf = SimpleNamespace(**{\n",
    "        'batch_size': 16,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 0.001,\n",
    "        'decay': 0.0,\n",
    "        \"uuid\": \"nodense-ae\",\n",
    "        \"encoded_name\": \"encoded\",\n",
    "    }) \n",
    "    conf = init_config(img_shape, conf, extra_conf)\n",
    "    ae, encoder = deeper_nodense(conf)\n",
    "    \n",
    "    if load:\n",
    "        load_weights(ae, conf.uuid)\n",
    "    if train:\n",
    "        train_ae(ae, conf, data_handler, save=True)\n",
    "    plot_prediction(ae, data_handler, save_name=conf.uuid)\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classifier training\n",
    "The classifier training function is very similar to the autoencoder classifier. The main difference is that we map the output values to one-hot encodings. Again, a configuration dictionary is provided for all of the hyper-parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(classifier, class_conf, data_handler, save=True):\n",
    "    batches = data_handler.get_train_batch_streamer(class_conf.batch_size)\n",
    "    batches_validation = data_handler.get_validation_batch_streamer(\n",
    "        class_conf.batch_size)\n",
    "    \n",
    "    # Map stream generators to one-hot encodings y \n",
    "    to_onehot = create_onehot_transformer(class_conf.num_classes)\n",
    "    batches_train_gen = map(to_onehot, pescador.tuples(batches, 'X', 'y'))\n",
    "    batches_valid_gen = map(to_onehot, pescador.tuples(batches_validation, 'X', 'y'))\n",
    "\n",
    "    checkpoints = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "        ]\n",
    "    \n",
    "    if save:\n",
    "        checkpoints.append(ModelCheckpoint(os.path.join(MODEL_DIR, class_conf.uuid + \".h5\"), monitor='val_acc',\n",
    "                                           save_weights_only=True,\n",
    "                                           verbose=1, mode='auto'))\n",
    "        checkpoints.append(CSVLogger(os.path.join(OUTPUT_DIR, class_conf.uuid + \".log\")))\n",
    "\n",
    "    classifier.fit_generator(\n",
    "        batches_train_gen,\n",
    "        steps_per_epoch=data_handler.number_of_train_batches,\n",
    "        epochs=class_conf.epochs,\n",
    "        verbose=1,\n",
    "        validation_data=batches_valid_gen ,\n",
    "        validation_steps=data_handler.number_of_val_batches,\n",
    "        callbacks=checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_classifier function is called in a classify training function, which specifies also a configuration dictionary and loads weights, if we are using a pre-training model. Again, to experiment with different hyper-parameters, you will need to change them here or in the higher function calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _classify(ae, load=True, extra_conf={}): \n",
    "    encoded_name = \"encoded\"\n",
    "    img_shape = data_handler.get_X_shape()\n",
    "    z_size = K.int_shape(ae.get_layer(encoded_name).output)[1:]\n",
    "    class_conf = SimpleNamespace(**{\n",
    "        'batch_size': 16,\n",
    "        'epochs': 150,\n",
    "        \"encoded_name\": encoded_name,\n",
    "        \"uuid\": \"nodense-classify\",\n",
    "        \"latent_dim\": z_size, # Depends on z dimensionality in AE.\n",
    "        \"num_classes\": 50,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"decay\": 0.0,\n",
    "        \"dropout\": 0.5,\n",
    "    }) \n",
    "    class_conf = init_config(img_shape, class_conf, extra_conf)\n",
    "    classifier = build_pretrained_classifier(ae, class_conf)\n",
    "    if load:\n",
    "        load_weights(classifier, class_conf.uuid)\n",
    "    else:\n",
    "        train_classifier(classifier, class_conf, labeled_data_handler)\n",
    "    return classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting it all together\n",
    "We have now defined the different training functions used for the different architectures. The final function run() first trains a supervised classifier without pre-trained weights. Then it trains an autoencoder, loads the pre-trained weights for the classifier and finally fine-tunes the classifier model using a semi-supervised learning approach. \n",
    "\n",
    "Even when trained on a GPU, the training time is far above 24 hours. If a wallclock limit is reached, it is possible to resume training with already trained weights by setting load=true for the semi-supervised model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(classifier):\n",
    "    tests_batches = labeled_data_handler.get_test_batch_streamer(16)\n",
    "    to_onehot = create_onehot_transformer(num_classes=50)\n",
    "    batches_gen = map(to_onehot, pescador.tuples(tests_batches, 'X', 'y'))\n",
    "    accuracy = classifier.evaluate_generator(batches_gen , steps=labeled_data_handler.number_of_test_batches)\n",
    "    print(accuracy)\n",
    "\n",
    "def run(extra_conf):\n",
    "    conf = {\n",
    "        'use_depool': True,\n",
    "        'hidden_units': 128,\n",
    "        'batch_size': 32, \n",
    "        'num_filters': 64, \n",
    "        'dropout': 0,\n",
    "        'use_batch_normalization': False, \n",
    "        'num_layers': 3\n",
    "    }\n",
    "    conf.update(extra_conf)\n",
    "    pooltype = 'unpool' if conf['use_depool'] else 'upsampl'\n",
    "    if DEBUG == True:\n",
    "        conf['epochs'] = 1\n",
    "\n",
    "    # Supervised classifier\n",
    "    conf['lambda_uuid'] = lambda uuid: \"cnn-{}-{}-bs{}-fs{}-d{}-bn{}-l{}-hu{}\".format(\n",
    "            pooltype, uuid, conf['batch_size'], conf['num_filters'], conf['dropout'],\n",
    "            conf['use_batch_normalization'], conf['num_layers'], conf['hidden_units'])\n",
    "    ae = autoencoder(load=False, train=False, extra_conf=conf)\n",
    "    classifier = _classify(ae, load=False, extra_conf=conf)\n",
    "    test_accuracy(classifier)\n",
    "\n",
    "    # Semi-supervised learning \n",
    "    conf['lambda_uuid'] = lambda uuid: \"{}-{}-bs{}-fs{}-d{}-bn{}-l{}-hu{}\".format(\n",
    "            pooltype, uuid, conf['batch_size'], conf['num_filters'], conf['dropout'], \n",
    "            conf['use_batch_normalization'], conf['num_layers'], conf['hidden_units'])\n",
    "    ae = autoencoder(load=False, train=True, extra_conf=conf)\n",
    "    classifier = _classify(ae, load=False, extra_conf=conf)\n",
    "    test_accuracy(classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-search for optimal hyper-parameters\n",
    "To run the different models from the handed-in paper, you can call the run function as following, using one of the setups from the models dictionary. The parameters are propagated down to the other configuration dictionaries and hence used for training and initialization of models. By default, the following will use the hyperparameters used for our best performing model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ArpeLarpe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Run with argument\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "models = [\n",
    "    {'batch_size': 64, 'num_filters': 64, 'dropout': 0.0},\n",
    "    # 3.125539016723633, 0.47499999999999998, 29/150, \n",
    "    {'batch_size': 32, 'num_filters': 64, 'dropout': 0.0}, \n",
    "    # 2.9574200391769407, 0.49249999999999999, 22/150, 7/50\n",
    "    # 2.622162959575653, 0.50749999999999995\n",
    "    {'batch_size': 16, 'num_filters': 64, 'dropout': 0.0},\n",
    "    # 4.0324129104614261, 0.41499999999999998, 29/150, 7/50\n",
    "    # 3.0564176845550537, 0.50249999999999995\n",
    "    {'batch_size': 16, 'num_filters': 64, 'dropout': 0.3},\n",
    "    # 2.6852602195739745, 0.33250000000000002, 25/150, 7/50\n",
    "    # 2.2190281200408934, 0.47749999999999998\n",
    "    {'batch_size': 16, 'num_filters': 64, 'dropout': 0.0, 'use_depool': False},\n",
    "    # 4.0324129104614261, 0.41499999999999998, 29/150, 7/50\n",
    "    # 15.836028633117676, 0.017500000000000002\n",
    "    # TODO: 'hidden_units': 128\n",
    "]\n",
    "\n",
    "if len(sys.argv) == 2:\n",
    "    run(models[int(sys.argv[1])])\n",
    "else:\n",
    "    import warnings\n",
    "    warnings.warn(\"Run with argument\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and visualizations\n",
    "The final section shows our results and visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
